---
title: "Manual_NPZD_model"
author: "Steven Pint, Viviana Otero, Patricia Cabrera, Laura Marquez and Gert Everaert \n Flanders Marine Institute Wandelaarkaai 7 Ostend 8400 Belgium"
date: "10/12/2021"
last update: "28/03/2022"
output: pdf_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.align = "left", echo = TRUE)
```

*This pdf file is also available as a Rmarkdown (Folder: Workspace > VRE Folders > Zoo-Phytoplankton_EOV > Modelling_phyto_and_zooplankton_interactions > Manual NPZD model > Manual_NPZD_model.Rmd). Store this file in your workspace and open it in Rstudio to have a interactive document.*

# Content Table
  1. Introduction
    1.1 Modelling Approach
    1.2 Input Data
    1.3 Study Area
  2. Preparation
    2.1 Join the virtual lab Zoo-Phytoplankton EOV
    2.2 Working Folder
    2.3 Input data
    2.4 Rscripts
    2.5 Rpackages
  3. NPZD Model
    3.1 Calibration: Run the NPZD model
    3.2 Calibration: Calculate the Root Mean Square Error
    3.3 Simulation: Run the NPZD model a second time
    3.4 Simulation: Calculate the Root Mean Square Error
    3.5 Simulation: Calculate the relative contribution of the environmental parameters to phytoplankton dynamics
    3.6 Visualisation: Creating graphs with the results
  4. References

# 1. Introduction
Marine phytoplankton is at the base of the marine food web and regulates functions in coastal ecosystems. Changes observed in the marine plankton community are expected to have a knock-on effect throughout the food web. Therefore, understanding how primary production changes through time and space is of key importance to better quantify the effects of human activities and their impact on the ocean.

With the methodology presented in this document, it is possible to analyse which factors drive the phytoplankton dynamics and how these factors change in space and time. This workflow is focused on the Belgian part of the North Sea (BPNS), located in the southern part of the North Sea, where high density of observations are available. Therefore the parametrization and visualizations shown here correspond only to this particular region. For other regions, this document can be used as a guideline to perform similar analyses. It is important to note that the principles behind the modelling approach are universally applicable. When applying the model to another area, recalibration of the model to the local conditions is required for optimal performance (cfr. section 3.1 and 3.2).

We start from describing the input data, Rscripts and functions that were used for this Nutrient-Phytoplankton-Zooplankton (NPZD) model. Then, this manual will guide you step by step on how to run the NPZD model. At the end, phyto- and zooplankton dynamics will be simulated and the relative contributions of the environmental parameters estimated.

## 1.1 Modelling Approach

The ecosystem model for Nutrient, Phytoplankton and Zooplankton was used to simulate changes in plankton density from 2014 to 2017 (Soetaert and Herman, 2009). This model describes daily changes in phyto- and zooplankton density based on abiotic parameters (Fig. 1). The variables in the model are expressed in mmol N m<-3> for nutrient, phytoplankton and zooplankton densities. Daily changes in these variables are expressed in  mmol N m<-3> d<-1>.

This model is useful to describe marine and freshwater systems. The state variables are nutrients, phytoplankton and zooplankton. The workflow focused on a marine system and followed a similar methodology as described in Everaert et al. (2015). Nutrients are defined as the total density of Dissolved Inorganic Nitrogen (DIN), Phosphate (PO~4~) and Silicate (SiO~4~). DIN is defined as the sum of NH~4~, NO~3~ and NO~2~.

![](C:/Users/stevenp/OneDrive - VLIZ/Documents/stevenp/BlueCloud2026/NPZD/BC2026/NPZD/Input data/NPZD.jpg)
*Figure 1*. Structure of the NPZD ecological model, with input data (grey ovals) and state variables (blue rectangles). Input data were taken from the LifeWatch Portal and the Flemish Monitoring Network. Generalized Additive Models (GAM) were used to obtain daily data for nutrients based on monthly and seasonal observations from the LifeWatch Portal. GAM were applied for the input data indicated with dash lines. (~/workspace/VREFolders/CarbonPlanktonDynamics/Manual NPZD model/ - on the BC2026 server) 


The changes in the state variables are defined by the flows between variables:

  - f1: net nutrient uptake of phytoplankton
  - f2: zooplankton grazing
  - f3: zooplankton faeces production
  - f4: zooplankton excretion
  - f5: zooplankton mortality

## 1.2 Input Data

The model requires the following input data: 

  1. Sea Surface Temperature (SST) in °C
  2. Nutrients (DIN, PO~4~, SiO~4~) in mmol N m<-3> (or P or Si equivalent)
  3. Threshold values for each parameter per region

For calibration and validation:

  1. Chlorophyll-a abundances (µgr Chl m<-3>)
  2. Zooplankton abundances (ind m<-3>)


## 1.3 Study Area

This workflow is focused on the Belgian part of the North Sea (BPNS; Fig. 2), located in the southern part of the North Sea. The following locations are the selected sampling stations used in the analyses presented in this document. We focused on nearshore, midshore and offshore regions. We analysed one nearshore station (code 130 in the LifeWatch campaings), one midshore (code 330) and seven offshore stations. The offshore stations were grouped to have a sufficient amount of data as the offshore region was sampled seasonnaly, whereas the other regions were sampled monthly.

![](C:/Users/stevenp/OneDrive - VLIZ/Documents/stevenp/BlueCloud2026/NPZD/BC2026/NPZD/Input data/map BPNS.png)
*Figure 2*. Location of the stations of interest in the Belgian Part of the North Sea. The black line indicates the Belgian Exclusive Economic Zone. The nearshore, midshore and offshore are <10km, 10km - 30km and >30km respectively in distance to the coast.


# 2. Preparation
## 2.1 Join the virtual lab Zoo-Phytoplankton EOV
In case you have not already, please create an account for the Blue Cloud (<https://blue-cloud.d4science.org/>). To join the virtual lab Zoo-Phytoplankton EOV, use the following link (<https://blue-cloud.d4science.org/web/zoo-phytoplankton_eov>) and click on "Access the VLab". Here you can open R Studio by clicking on it in the blue bar.
The next step is to create new folders and copy the input data and Rscripts to your workspace. 


## 2.2 Working Folder
Let's create some working folders first. This is done to structure the work and make sure that the model can find all data it needs. By following this structure, less adjustments to directories are needed. We will create folders in our workspace to store our script, data and output. Input data and Rscripts can be copied from the 'Manual NPZD folder' to our working folder.

```{r,echo=FALSE}
# create a working folder on your personal workspace to store your input data, rscripts and results
dir.create("~/workspace/NPZD") # create a working folder for your results

# set working directory
wd =  "C:/Users/stevenp/OneDrive - VLIZ/Documents/stevenp/BlueCloud2026/NPZD/BC2026/NPZD/" # change for on BC2026 server - ~/workspace/NPZD/

# copy and past files using R
current_folder_input_data <- "~/workspace/VREFolders/Zoo-Phytoplankton_EOV/Modelling_phyto_and_zooplankton_interactions/Manual NPZD model/Input data"
current_folder_Rscripts <- "~/workspace/VREFolders/Zoo-Phytoplankton_EOV/Modelling_phyto_and_zooplankton_interactions/Manual NPZD model/Rscripts"
new_folder <- "~/workspace/NPZD"

file.copy(current_folder_input_data, new_folder, recursive=TRUE)
file.copy(current_folder_Rscripts, new_folder, recursive=TRUE)

# create a folder to store your results
dir.create(paste0(wd, "Output")) # create an output folder for your results
dir.create(paste0(wd, "Output/Intermediate results")) # create an subfolder for your intermediate results
dir.create(paste0(wd, "Output/Final results")) # create an subfolderfolder for your final results
dir.create(paste0(wd, "Output/Final results/NPZD")) # create an subfolderfolder for your final NPZD results
dir.create(paste0(wd, "Output/Final results/Relative contributions")) # create an subfolderfolder for your relative contributions results
```
Now, there should be a folder called 'NPZD' in your workspace, containing the subfolders 'Input data','Rscripts' and 'Output'. These subfolders contain subfolders and data or Rscripts. Having these files in our workspace, makes it possible to run the NPZD model.


## 2.3 Input Data
The input data that is required to perform a NPZD model is:
  1. Sea Surface Temperature (SST) in °C
  2. Nutrients (DIN, PO~4~, SiO~4~) in mmol N m<-3> (or P or Si equivalent)
  3. Threshold values for each parameter per region (provided by region)
  
The input data should be daily for the first two parameters. If this is not the case for your input data, we would like to refer to some guidelines available on https://m-clark.github.io/generalized-additive-models/ and https://petolau.github.io/Analyzing-double-seasonal-time-series-with-GAM-in-R/ to create input data that is daily.

Further, before running the NPZD model, the input data should be in the right format, i.e. .csv files, same order of the columns and same column names (Table 1), and the input data should be in the same units (see above). 

*Table 1*. An example of how the the input data format looks like.
```{r, echo=FALSE, results='asis'}
library(knitr)
InputData_example <- read.csv(paste0(wd, "Input data/inputData_NPZD_stationnearshore.csv"), header = TRUE)
kable(head(InputData_example), align = "c")
```


## 2.4 Rscripts
The Rscripts in the subfolder 'NPZD' under Rscript contain functions that are used to run the model. They are called and loaded automatically when running this Rmarkdown.
In the other subfolder 'GAM', there are two types of Rscripts, i.e. Rscripts containing functions and working Rscripts. The latter are used to work in and adjust to your needs, whereas the former contains functions that are used to run the model. The functions Rscripts can be recognized by their their name starting with "function".

The Rscripts should be stored in the Rscripts folder ("~/workspace/NPZD/Rscripts") that we created in the previous section.


## 2.5 Rpackages
Before we start running the NPZD model, we want to make sure all the needed Rpackages are properly installed. With the code below, we check if the Rpackages that we will use are installed. If this is not the case, it will install the Rpackages that are missing. 
```{r, echo=FALSE}
source(paste0(wd, "Rscripts/NPZD/function_installed_packages.R"))
install_needed_packages()
```


# 3. NPZD Model

If you are interested in creating phyto- and zooplankton dynamics for a similar area than our study area, then you can skip forward to section 3.3. If you are working in a new area, it is adviced to recalibrate the model to your environmental settings and start at section 3.1. The Rmarkdown has the nearshore conditions as initial settings. Adjust or select the rigth code when working in another region.  

Let's provide the code of the station, where the samples were taken. This station code is later used when working with the observations. 
```{r, echo=FALSE}
station_code <- "130" # nearshore: "130", midshore: "330" and offshore: c("LW01", "LW02", "W10", "W09", "W08", "421", "435") in this example
```

## 3.1 Calibration: Run the NPZD model
We can start calibrating the NPZD model. First, we run the NPZD with threshold values aggregated from literature for each parameter.

*Used Rscript:*
  - *functions_NPZD_parallel_iter1.R*

We start with loading the associated functions and Rpackages, that we will need during this first step. 
```{r, echo=FALSE}
#NPZD Model - Parallel version
#Iteration 1

##########
source(paste0(wd, "Rscripts/NPZD/functions_NPZD_parallel_iter1.R"))
library(parallel)
library(foreach)
library(doParallel)
```
Then we load our input data for our station of interest. In this example it is for station 'nearshore'. We provide the name of the station of interest to create output with a according name in a later phase. So, we provide the name of the station of interest and load our input file. If your input data file has a different name, you can adjust it between the "" in the read.csv function.
```{r, echo=FALSE}
###########################
#get data from station of interest

station_int <- 'nearshore'
#station_int <- 'midshore'
#station_int <- 'offshore'
inputData <- read.csv(paste0(wd, "Input data/NPZD/inputData_NPZD_station", station_int,".csv"))
```
Now, we decide how many iterations the NPZD model will do. For our publication, we selected 5000 as a good balance between quality and processing time. It is possible to change the number of iterations, for example 500 is suitable for a first trial. Then, we create 500 initial combinations for the parameters based on their threshold values in literature. In addition, we write a .csv file with the possible parameters and store it in the output folder.
```{r, echo=FALSE}
#Simulation parameters, variations based on random values
###########create file################
numSimulations <- 500 #or 5000 

possible_parameters <- create_parameters(numSimulations, station_int)
dir.create(paste0(wd, "Output/Intermediate results/station", station_int,"_iter1"))
write.csv(possible_parameters, paste0(wd, "Output/Intermediate results/station", station_int,"_iter1/possible_parameters_iter1.csv"))
```
Now, it is time to run the NPZD model for each combination of parameters, i.e. 500 times. This will be done in parallel to speed up the process. This means that we will use several cores of the server in this case. You can specify how many cores you want to use in the line containing "cores <- num_cores - 1". We left one core free, but you can choose to leave more cores free. It necessary to leave at least 1 core free as this core will be used to manage the other cores.
The number in the NPZD_run() functions is the number of days that will be simulated. In our example it is 2557 days, i.e. 7 years is 7 times 365 days and we have two leap years in our time series, so we have to add two days extra making 2557 days in total. Don't forget to adjust if you have a different time series. 
The output of the model is stored in a subfolder ("stationnearshore_iter1") of the output folder, and will have a name such as "detailed_simulation_2555.csv"
```{r, echo=FALSE}
######################
#Simulate NPZD model for each combination of parameters i

#Parallel

num_cores <- detectCores()
cores <- num_cores - 1
cl <- makeCluster(cores)
registerDoParallel(cl)

text_NPZD <- foreach(i = 1:numSimulations, .combine = rbind) %dopar% {
    tryCatch({
      NPZD_run(2557, possible_parameters[i,], station_int, inputData)   
    }, error = function(e) return(paste0("Error in NPZD iteration: ", i, 
                                         ", caused error: ", e)))        
  
  }


stopImplicitCluster()
stopCluster(cl)
```


## 3.2 Calibration 2: Calculate the Root Mean Square Error
Time to see how well the model simulated phyto-and zooplankton dynamics. In this step, we compare the model results with observation data. This is done by calculating the Root Mean Square Error (RMSE). The best fitted models, i.e. the models of which the total RMSE was the lowest, will be selected to update the threshold value of the model's parameters. By doing this, we optimise the model to make realistic predictions. The parameters of the best 10% simulations are used to optimise the threshold values of the parameters. Then, we can run the model for a second time for 500 or 5000 iterations.

*Used Rscript:*
  - *functions_error_v1.R*
  
We start by loading the functions that we will use in this step.
```{r, echo=FALSE}
#Error analysis NPZD model
source(paste0(wd, "Rscripts/NPZD/functions_error_v1.R"))
```
Then, we define our station of interest and the number of simulation that were performed.
```{r, echo=FALSE}
###################################################
## Process error data

num_station <- "nearshore"
#num_station <- "midshore"
#off_shore_stations <- c("LW01", "LW02", "W10", "W09", "W08", "421", "435")
#num_station <- "offshore"

num_sim <- 500 #or 5000
```
We load the phytoplankton data, i.e. chlorophyll-a observations, which was collected from LifeWatch (https://rshiny.lifewatch.be/station-data/).
The data looks like the example below.
```{r, echo=FALSE, results='asis'}
library(knitr)
InputData_example <- read.csv(paste0(wd, "Input data/NPZD/abiotic_pigment_2020.csv"), header = TRUE)
kable(head(InputData_example), align = "c")
```
After loading the phytoplankton data, we define the time interval of observation that we want to use to compare our simulations with. The time interval can be set in the line "chla<-subset(subset(chla, Time >"2013-12-31"), Time < "2018-01-01")".
```{r, echo=FALSE}
###################################################
#Calculate error

#Get observations of Chla and zooplankton
observations<-read.csv(paste0(wd, "Input data/NPZD/abiotic_pigment_2020.csv"))
levels(observations$Station)[levels(observations$Station) == station_code] <- "nearshore"
#levels(observations$Station)[levels(observations$Station) == station_code] <- "midshore"
#levels(observations$Station)[levels(observations$Station) == station_code] <- "offshore"

#Observations from LifeWatch
#Station, Date, Chlorophyll_a
chla <- observations[,c(1, 2, 6)]
chla<-chla[which(chla$Station==num_station),]

#chla<-chla[chla$Station %in% off_shore_stations,]

chla$Time<-as.Date(chla$Time, "%d-%m-%y")
chla<-subset(subset(chla, Time >"2013-12-31"), Time < "2018-01-01")
chla <- chla[!is.na(chla$Chlorophyll_a),]
```
We do the same for zooplankton. So, we load the zooplankton data (see example below) from LifeWatch (https://rshiny.lifewatch.be/zooscan-data/) and define the time interval at line "zoo_observations<-subset(subset(zoo_observations, Date >"2013-12-31"), Date < "2018-01-01")". The output is stored in the "Input data"" folder as the output will be used to compare simulations with observations. In the case of offshore stations, run the line of code "zoo_observations<-zoo_observations[which(zoo_observations$Station %in% off_shore_stations),]" without a "#" before it.
```{r, echo=FALSE, results='asis'}
library(knitr)
InputData_example <- read.csv(paste0(wd, "Input data/NPZD/zooscan.csv"), header = TRUE)
kable(head(InputData_example), align = "c")
```

```{r, echo=FALSE}
#Zooplankton data
zoo_observations<-read.csv(paste0(wd, "Input data/NPZD/zooscan.csv")) # only zooscan data
zoo_observations<-zoo_observations[which(zoo_observations$Station == station_code),]

#zoo_observations<-zoo_observations[which(zoo_observations$Station %in% off_shore_stations),]

zoo_observations$Date<-as.Date(zoo_observations$Date, format = "%Y-%m-%d")
zoo_observations<-subset(subset(zoo_observations, Date >"2013-12-31"), Date < "2018-01-01")

zoo_converted <- convertZoo(zoo_observations, num_station)
zoo_perTaxa <- as.data.frame(zoo_converted[[1]])
zoo_aggregated <- as.data.frame(zoo_converted[[2]])
write.csv(zoo_aggregated, paste0(wd, "Input data/NPZD/zoo_converted_station", num_station, ".csv"))
```
Now, we remove outliers in the zooplankon data (only for nearshore).
```{r, echo=FALSE}
###
zoo_aggregated <- read.csv(paste0(wd, "Input data/NPZD/zoo_converted_station", num_station, ".csv"))
zoo_aggregated <- zoo_aggregated[,c(-1)]
zoo_aggregated$Date <- as.Date(zoo_aggregated$Date)

#Remove outlier station nearshore
ggplot(data=zoo_aggregated, aes(x=as.Date(Date), y = n_mol)) + geom_line()
quantile(zoo_aggregated$n_mol)
zoo_aggregated <- zoo_aggregated[which(zoo_aggregated$n_mol < 0.5),]
```
The calculateErrors() function stores all simulations that were not successfull, which is used to remove them. 
Here, we calculate the total RMSE of each model run. With the total RMSE, we can find the best model parameters, i.e. the one of which the total RMSE is the lowest. 
```{r, echo=FALSE}
simulations_error <- calculateErrors(num_station, chla, zoo_aggregated, num_sim)

#############################
#Extract best simulations
#############################
simulations_error <- simulations_error[!is.na(simulations_error$ID_sim),]
numSim <- nrow(simulations_error)
```
The best ten percent (0.1) of simulations will be selected. It is possible to specify the percentage to your choice.
```{r, echo=FALSE}
#Select the best simulation based on a percentage
numSim_select <- trunc(numSim*0.1)

#analysis error based on RMSE
order_error <- simulations_error[order(simulations_error$total_rmse),]
min_error_rmse <- order_error[c(1:numSim_select),]
quantile(min_error_rmse$total_rmse)
```
We can visualise the simulations in combination with the observations. This can be done for a daily time series and for monthly. We can visually check if the model did a good job.
```{r, echo=FALSE}
#####Create graphs of results with best simulations as time series
#based on RMSE
bestSim_graphs_rmse <- createBestSim(min_error_rmse$ID_sim, num_station, chla, zoo_aggregated)

#####Create monthly comparison
simulations_best_rmse <- bestSimulations(min_error_rmse$ID_sim, num_station)
graph_best_rmse_monthly <- monthlyBestSimulations(simulations_best_rmse, chla, zoo_aggregated)

```
After a visual control, we select new parameter threshold values based on the 10% best runs from the previous iteration to optimise the model. To further optimise the model, we select the 10% best parameter values for spring conditions and the 10% best parameter values for summer-autumn conditions.
```{r, echo=FALSE}
#######################################
#Select parameters for next iteration
#######################################

#Punishment in spring and summer months
simulations_error_penalized <- increaseError_springSummer(simulations_error, num_station, chla)

#Total_rmse_2 for spring
order_error2 <- simulations_error_penalized[order(simulations_error_penalized$total_rmse_2),]
min_error_rmse2 <- order_error2[c(1:numSim_select),]

bestSim_graphs_spring <- createBestSim(min_error_rmse2$ID_sim, num_station, chla, zoo_aggregated)

bestSim_spring <- bestSimulations(min_error_rmse2$ID_sim, num_station)
graph_monthly_spring <- monthlyBestSimulations(bestSim_spring, chla, zoo_aggregated)

#Compare selection with non penalized set
comparison_spring<-data.frame(matrix(nrow = numSim_select, ncol = 2))
names(comparison_spring)[1] <- "rmse1_ID"
names(comparison_spring)[2] <- "in_rmse2"

comparison_spring$rmse1_ID <- min_error_rmse$ID_sim
comparison_spring$in_rmse2 <- comparison_spring$rmse1_ID %in% min_error_rmse2$ID_sim

count(comparison_spring$in_rmse2)

#Total_rmse_3 for summer
order_error3 <- simulations_error_penalized[order(simulations_error_penalized$total_rmse_3),]
min_error_rmse3 <- order_error3[c(1:numSim_select),]

bestSim_graphs_summer <- createBestSim(min_error_rmse3$ID_sim, num_station, chla, zoo_aggregated)

bestSim_summer <- bestSimulations(min_error_rmse3$ID_sim, num_station)
graph_monthly_summer <- monthlyBestSimulations(bestSim_summer, chla, zoo_aggregated)

#Compare selection with non penalized set
comparison_summer<-data.frame(matrix(nrow = numSim_select, ncol = 2))
names(comparison_summer)[1] <- "rmse1_ID"
names(comparison_summer)[2] <- "in_rmse3"

comparison_summer$rmse1_ID <- min_error_rmse$ID_sim
comparison_summer$in_rmse3 <- comparison_summer$rmse1_ID %in% min_error_rmse3$ID_sim

count(comparison_summer$in_rmse3)
```
After selecting the 10% best parameters, we create two new files, one with the parameters threshold values for the spring conditions and one for summer-autumn conditions. The new files are stored in the data folder and will be used as input in the next model iteration.
```{r, echo=FALSE}
##########################################
#Create file of parameters for iteration 2
#########################################

#Parameters for January to June
newParameters_spring <- createNewParameters_def(min_error_rmse2, 2,4)
write.csv(newParameters_spring, paste0(wd, "Input data/NPZD/parameters_5000_iter2_spring_station", num_station, ".csv"))

#Parameters for July to December
newParameters_summer <- createNewParameters_def(min_error_rmse3, 2,4)
write.csv(newParameters_summer, paste0(wd, "Input data/NPZD/parameters_5000_iter2_summer_station", num_station, ".csv"))
```

## 3.3 Simulation: Run the NPZD model
Let's run the NPZD model a second time, but now we use the updated threshold values created in the previous iteration. 

*Used Rscript:*
  - *functions_NPZD_parallel_iter2_artifact.R*

First, we start again by loading the functions and Rpackages needed to run the model. 
```{r, echo=FALSE}
#NPZD Model - Parallel version
#Iteration 2

##########
source(paste0(wd, "Rscripts/NPZD/functions_NPZD_parallel_iter2_artifact.R"))
library(parallel)
library(foreach)
library(doParallel)
```
Similar to the first step (3.1), we provide the station of interest and load the input data. Further, we decide how many iterations the NPZD model will do. For our publication, we selected 5000 as a good balance between quality and processing time. Then, we created 5000 starting combinations for the parameters based on their threshold value defined by the previous iteration. This is done in twofold, once for the spring condition and once for the summer-autumn conditions. In addition, we write .csv files for both conditions, with the possible parameters and store it in the output folder. 
```{r, echo=FALSE}
###########################
#Get data generated using GAM for DIN, PO4, SiO4, and temperature
#get data from station of interest

station_int <-"nearshore"
#station_int <- "midshore"
#station_int <-"offshore"
inputData <- read.csv(paste0(wd, "Input data/NPZD/inputData_NPZD_station", station_int,".csv"))

#Simulation parameters, variations based on random values
###########create file################
numSimulations <- 500 #or 5000

#Create parameters for penalized values - from January to 21 June
parameters_values1 <- read.csv(paste0(wd, "Input data/NPZD/parameters_5000_iter2_spring_station", station_int, ".csv"))
possible_parameters_1 <- create_parameters(numSimulations, parameters_values1, 1)
dir.create(paste0(wd, "Output/Final results/NPZD/station", station_int,"_iter2"))
write.csv(possible_parameters_1, paste0(wd, "Output/Final results/NPZD/station", station_int,"_iter2/possible_parameters_spring_iter2.csv"))


#Create parameters - from 22 June to December
parameters_values2 <- read.csv(paste0(wd, "Input data/NPZD/parameters_5000_iter2_summer_station", station_int, ".csv"))
possible_parameters_2 <- create_parameters(numSimulations, parameters_values2, 2)
write.csv(possible_parameters_2, paste0(wd, "Output/Final results/NPZD/station", station_int,"_iter2/possible_parameters_summer_iter2.csv"))
```
For the second time, we will run the NPZD model for each combination of parameters, i.e. 500 or 5000 times, of the spring and summer-autumn conditions. This will also be done in parallel to speed up the process. This means that we will use several cores of the server in this case. You can specify how many cores you want to use in the line containing "cores <- num_cores - 1". We left 1 core free, but you can choose to leave more cores free. It necessary to leave at least 1 core free as this core will be used to manage the other cores.
The number in the NPZD_run() functions is the number of days that will be simulated. Don't forget to adjust if you have a different time series. 
The output of the model is stored in a subfolder ("stationnearshore_iter2") of the output folder, and will have a name such as "detailed_simulation_2555.csv"
```{r, echo=FALSE, include=FALSE}
######################
#Simulate NPZD model for each combination of parameters i

#Parallel

num_cores <- detectCores()
cores <- num_cores - 1
cl <- makeCluster(cores)
registerDoParallel(cl)

text_NPZD <- foreach(i = 1:numSimulations, .combine = rbind, .packages = "lubridate") %dopar% {
  tryCatch({
    NPZD_run(2557, possible_parameters_1[i,], possible_parameters_2[i,], station_int, inputData)   
  }, error = function(e) return(paste0("Error in NPZD iteration: ", i, 
                                       ", caused error: ", e)))        
  
}

stopImplicitCluster()
stopCluster(cl)
```

## 3.4 Simulation: Calculate the Root Mean Square Error
Again, we calculate the RMSEs to see how well the model simulated phyto- and zooplankton dynamics. Now, we will select the best 10% to simulate the plankton dynamics and estimate the uncertainty on the simulations, i.e. provide a 95% confidence interval. 

*Used Rscript:*
  - *functions_error_iter2*

We load our functions that we will use first.
```{r, echo=FALSE}
#Error analysis NPZD model
source(paste0(wd, "Rscripts/NPZD/functions_error_iter2.R"))

```
Specify our station of interest and the number of simulation that were performed.
```{r, echo=FALSE}
###################################################
## Process error data
num_station <- "nearshore"

num_sim <- 500 # or 5000
```
We load the phytoplankton data, i.e. chlorophyll-a observations, which was collected from LifeWatch (https://rshiny.lifewatch.be/station-data/), similar to 3.2. After loading the phytoplankton data, we define the time interval of observation that we want to use to compare our simulations with. The time interval can be set in the line "chla<-subset(subset(chla, Time >"2013-12-31"), Time < "2018-01-01")".
```{r, echo=FALSE}
###################################################
#Calculate error

#Get observations of Chla and zooplankton
observations <- read.csv(paste0(wd, "Input data/NPZD/abiotic_pigment_2020.csv"))
levels(observations$Station)[levels(observations$Station) == station_code] <- "nearshore"
#levels(observations$Station)[levels(observations$Station) == station_code] <- "midshore"
#levels(observations$Station)[levels(observations$Station) == station_code] <- "offshore"

#Observations from LifeWatch
#Station, Date, Chlorophyll_a
chla <- observations[,c(1, 2, 6)]
chla<-chla[which(chla$Station==num_station),]

#chla<-chla[chla$Station %in% off_shore_stations,]

chla$Time<-as.Date(chla$Time, "%d-%m-%y")
chla <- subset(subset(chla, Time >"2013-12-31"), Time < "2018-01-01")
chla <- chla[!is.na(chla$Chlorophyll_a),]
```
We do the same for zooplankton. So, we load the zooplankton data (see example below) from LifeWatch ((https://rshiny.lifewatch.be/zooscan-data/)) that was produced in 3.2.
Then, we remove outliers in the zooplankon data (only for nearshore). The calculateErrors() function stores all simulations that were not successfull, which is used to remove them.
```{r, echo=FALSE, include=FALSE}
#Zooplankton data
zoo_observations <- read.csv(paste0(wd, "Input data/NPZD/zoo_converted_station", num_station, ".csv"))

#remove outliers only for station nearshore
zoo_observations <- zoo_observations[which(zoo_observations$n_mol < 0.5),]

simulations_error <- calculateErrors(num_station, chla, zoo_observations, num_sim)
```
We calculate the total RMSE for the second iteration compared to the observation. The 10% best simulations are then selected to calculate and visualise phyto- and zooplankton dynamics. Here it is also possible to adjust the percentage (0.1) to your choice. The results are stored in a subfolder ("stationnearshore_iter2") of the output folder.
```{r, echo=FALSE}
#############################
#Extract best simulations
#############################
simulations_error <- simulations_error[!is.na(simulations_error$ID_sim),]
numSim <- nrow(simulations_error)

#Select the best simulation based on a percentage
numSim_select <- trunc(numSim*0.10)

#analysis error based on RMSE
order_error <- simulations_error[order(simulations_error$total_rmse),]
min_error_rmse <- order_error[c(1:numSim_select),]
quantile(min_error_rmse$total_rmse)

#####Create graphs of results with best simulations as time series
#based on RMSE
bestSim_graphs_rmse <- createBestSim(min_error_rmse$ID_sim, num_station, chla, zoo_observations)
```
Again, we can visually check how the model performed.
```{r, echo=FALSE}
#####Create monthly comparison
simulations_best_rmse <- bestSimulations(min_error_rmse$ID_sim, num_station)
graph_best_rmse_monthly <- monthlyBestSimulations(simulations_best_rmse, chla, zoo_observations)
```

## 3.5 Simulation: Calculate the relative contribution of the environmental parameters to phytoplankton dynamics
In this step, we will use the best 5% of simulations to estimate the relative contribution of the environmental parameters to phytoplankton dynamics. Only the best 5% are used to decrease computational effort.

*Used Rscript:*
  - *functions_relativeContributions_iter2.R*

We load the function and Rpackages that will be used in this script.
```{r, echo=FALSE}
####################################
#Relative contributions calculation
source(paste0(wd, "Rscripts/NPZD/functions_relativeContributions_iter2.R"))
library(viridis)
```
We specify our station of interest and load our results with the total RMSE from the subfolder in the output folder. We select only the 5% best simulations to calculate the relative contribution of the key drivers of primary production.
```{r, echo=FALSE}
#Select station and iteration of interest
#num_station <- "offshore"
#num_station <- "midshore"
num_station <- "nearshore"

#Load error per simulation
error_log <- read.csv(paste0(wd, "Output/Final results/NPZD/station", num_station, "_iter2/errors_parameters_iter2.csv"))
error_log <- error_log[which(!is.na(error_log$total_rmse)),]

#order simulations from lowest to highest error
order_error <- error_log[order(error_log$total_rmse),]

#select best simulations (10% or 5%)
percentage <- 0.05
numSim <- trunc(percentage * nrow(order_error))
#numSim<-100
selectedSim <- order_error[1:numSim,]
```
Finally, we calculate the relative contribution of the key drivers, i.e. nutrients (DIN, PO~4~ and SiO~4~), SST, PAR and zooplankton grazing. The results are stored in a subfolder ("relativeContributions_smooth") in the output folder and named for example "contributionsNormalized_month_stationnearshore.csv".
```{r, echo=FALSE}
#This function calculates the relative contributions per month per simulation
relativeContributions <- calculateContributions(selectedSim, num_station)

write.csv(relativeContributions, paste0(wd, "Output/Final results/Relative contributions/relativeContributions_month_station", num_station, ".csv"))

contributions_aggregated <- aggregate(relativeContributions$Contribution, by=list(relativeContributions$Date, relativeContributions$Type), FUN="mean",  data = relativeContributions)

names(contributions_aggregated)[1] <- "Date"
names(contributions_aggregated)[2] <- "Type"
names(contributions_aggregated)[3] <- "Contribution"

#Normalize data because of the median approximation
contributions_plot <- normalizeData(contributions_aggregated)

write.csv(contributions_plot, paste0(wd, "Output/Final results/Relative contributions/contributionsNormalized_month_station", num_station, ".csv"))
```

## 3.6 Visualisation: Creating graphs with the results
Now that everything is simulated, calculated and estimated, it's time to visualise our results. We provide a couple of examples on how the results can be visualised.

*Used Rscripts:*
  - *functions_graphsManuscript_v1.R*
  - *functions_relativeContributions_iter2.R*

We load the functions that are necessary to run this script. 
```{r, echo=FALSE}
source(paste0(wd, "Rscripts/NPZD/functions_graphsManuscript_v1.R"))
source(paste0(wd, "Rscripts/NPZD/functions_relativeContributions_iter2.R"))
library(viridis)
theme_set(theme_bw()) # this function is to have a white background in the plots
```
Then we process some results to get the phyto- and zooplankton dynamics ready to visualise, e.g. select the best simulations for each region.
```{r, echo=FALSE, warning = FALSE}
###########################
# visualisation
##########################

station <- "nearshore"
#Station nearshore
#This function extracts the details of the 10% best simulations for station nearshore
#See more details in the script of "functions_graphsManuscript_v1.R"
#This function takes a while to run
#The function returns a list of monthly values per simulation
bestSim_station <- getBestSim(station)

#Only get first element of the list, that is a dataframe with the values of Chlorophyll-a density
#for the best 10% best simulations

chla_station <- bestSim_station[[1]]
#This data frame has the average value per date, the previous one has all the values not aggregated
chla_station_avg <- aggregate( .~roundDate, data = chla_station, FUN = function(x) mean(as.numeric(as.character(x))))
chla_station_avg$Type <- "Chlorophyll a"
chla_station$Type <- "Chlorophyll a"

#Only get the second element of the list, that is a dataframe with the values of zooplankton density
#for the best 10% best simulations

zoo_station <- bestSim_station[[2]]
zoo_station_avg <- aggregate( .~roundDate, data = zoo_station, FUN = function(x) mean(as.numeric(as.character(x))) )
zoo_station_avg$Type <- "Zooplankton"
zoo_station$Type <- "Zooplankton"

temp_station <- chla_station_avg[,c(1,3,6)]
temp_station_2 <- zoo_station_avg[,c(1,3,6)]

names(temp_station)[2] <- "Plankton"
names(temp_station_2)[2] <- "Plankton"

combine_station <- rbind(temp_station, temp_station_2)

#Summary per Date
chla_station_avg <- chla_station_avg[,c(1,3,4,5)]

all_chla <- chla_station_avg
all_chla_complete <- chla_station

names(all_chla)[2] <- "Plankton"
names(all_chla_complete)[3] <- "Plankton"
  
zoo_station_avg <- zoo_station_avg[,c(1,3,4,5)]

all_zoo <- zoo_station_avg
all_zoo_complete <- zoo_station

names(all_zoo)[2] <- "Plankton"
names(all_zoo_complete)[3] <- "Plankton"

all_output_avg <- rbind(all_chla, all_zoo)
all_output_avg$Station <- station
all_output_complete <- rbind(all_chla_complete, all_zoo_complete)
```
After the processing, we can visualise the phyto-and zooplankton dynamics together in one plot by region.
```{r, echo=FALSE}
all_zoo_2nd_scale <- all_zoo
all_zoo_2nd_scale$Plankton <- log(all_zoo_2nd_scale$Plankton+1)*10
all_chla_1st_scale <- all_chla
all_chla_1st_scale$Plankton <-log(all_chla_1st_scale$Plankton+1)
all_chla_zoo_1st_2nd_scale <- rbind(all_chla_1st_scale,all_zoo_2nd_scale)

comparisonGraphAvg_station_chla_zoo <- ggplot() +
  geom_line(data = all_chla, aes(x = as.Date(roundDate), y = log(Plankton+1)), size = 1) + 
  geom_line(data = all_zoo, aes(x = as.Date(roundDate), y = log(Plankton+1)*10),colour = 'grey', size = 1, linetype = "dashed") + 
  scale_x_date(date_breaks = "3 months", date_labels =  "%b %Y", expand = c(0, 0)) +
  scale_y_continuous(name = expression(paste("Chlorophyll a (log (mg Chla ", m^-3," + 1))")), sec.axis = sec_axis( trans=~./10, 
                      name=expression(paste("Zooplankton (log (mmol N ", m^-3," + 1))")))) +
  theme(text = element_text(size=10), axis.text.x=element_text(angle=90, hjust=1)) +
  theme(legend.position = "bottom",legend.direction = "horizontal",legend.background = element_rect(),legend.title = element_blank(),
        legend.text = element_text(size=12),legend.text.align = 0,axis.text.x = element_text(colour="black",size=11.5),
        axis.text.y = element_text(colour="black",size=14), axis.title.x = element_text(colour="black",size=16,face = 'bold'),
        axis.title.y = element_text(colour="black",size=16,face = 'bold'), strip.text = element_text(size = 12))+
  labs(x = "Date", y= expression(paste("Zooplankton (log (mmol N ", m^-3," + 1))")))

comparisonGraphAvg_station_chla_zoo
```
We can visualise the phytoplankton dynamics separately.
```{r, echo=FALSE}
########################################
#Graph Chla
# create chla observation subset
chla_obs_station <- subset(chla, Station==station)

# calculate min and max for uncertainty ribbon used in plot chla and zooplankton
values_min <- aggregate( Plankton~roundDate + Station +Type, data = all_output_complete, FUN = min )
values_min$Limit <-"min"
values_max <- aggregate( Plankton~roundDate + Station +Type, data = all_output_complete, FUN = max )
values_max$Limit <-"max"
values_merged <- merge(values_min,values_max, by= c("roundDate", "Station", "Type"))
values_merged <- values_merged[,c(1,2,3,4,6)]
names(values_merged)[4] <- "Min"
names(values_merged)[5] <- "Max"
values_merged_chla <- values_merged[which(values_merged$Type == "Chlorophyll a"),]
values_merged_zoo <- values_merged[which(values_merged$Type == "Zooplankton"),]

# create plot
comparisonGraphAvg_station_chla <- ggplot() +
  geom_line(data = all_chla, aes(x = as.Date(roundDate), y = log(Plankton+1)), size = 1) + 
  geom_ribbon(data = values_merged_chla, aes(x=as.Date(roundDate), ymin = log(Min+1), ymax = log(Max+1)), fill = 'black',  alpha = 0.15) +
  geom_point(data=chla_obs_station, aes(x=as.Date(Time), y = log(Chlorophyll_a+1))) +
  scale_x_date(date_breaks = "3 months", date_labels =  "%b %Y", expand = c(0, 0)) +
  theme(text = element_text(size=10), axis.text.x=element_text(angle=90, hjust=1)) +
  theme(legend.position = "none",legend.direction = "vertical",legend.background = element_rect(),legend.title = element_text(size=14,face = 'bold'),
        legend.text = element_text(size=12),legend.text.align = 0,axis.text.x = element_text(colour="black",size=11.5),
        axis.text.y = element_text(colour="black",size=14), axis.title.x = element_text(colour="black",size=16,face = 'bold'),
        axis.title.y = element_text(colour="black",size=16,face = 'bold'), strip.text = element_text(size = 12))+
  labs(x = "Date", y = expression(paste("Chlorophyll a (log (mg Chla ", m^-3," + 1))")))

comparisonGraphAvg_station_chla
```
We can visualise the zooplankton dynamics separately by region.
```{r, echo=FALSE}
#zooplankton
zoo_obs_station <- zoo_observations

comparisonGraphAvg_station_zoo <- ggplot() +
  geom_line(data = all_zoo, aes(x = as.Date(roundDate), y = log(Plankton+1)), size = 1) + 
  geom_ribbon(data = values_merged_zoo, aes(x=as.Date(roundDate), ymin = log(Min+1), ymax = log(Max+1)),  alpha = 0.15) +
  geom_point(data=zoo_obs_station, aes(x=as.Date(Date), y = log(n_mol+1))) +
  scale_x_date(date_breaks = "3 months", date_labels =  "%b %Y", expand = c(0, 0)) +
  theme(text = element_text(size=10), axis.text.x=element_text(angle=90, hjust=1)) +
  theme(legend.position = "none",legend.direction = "vertical",legend.background = element_rect(),legend.title = element_text(size=14,face = 'bold'),
        legend.text = element_text(size=12),legend.text.align = 0,axis.text.x = element_text(colour="black",size=11.5),
        axis.text.y = element_text(colour="black",size=14), axis.title.x = element_text(colour="black",size=16,face = 'bold'),
        axis.title.y = element_text(colour="black",size=16,face = 'bold'), strip.text = element_text(size = 12))+
  labs(x = "Date", y= expression(paste("Zooplankton (log (mmol N ", m^-3," + 1))")))

comparisonGraphAvg_station_zoo
```
The relative contribution can also be visualised. This example gives the relative contribution of each parameter for primary production by region.

Now, we can have a look at the relative contribution by visualising it with the following code.
```{r, echo=FALSE}
########################################################################
# relative contributions 
# Let's prepare our data for the graph
  relativeContributions_station <- read.csv(paste0(wd, "Output/Final results/Relative contributions/relativeContributions_month_station",station,".csv"))

  relativeContributions_station$Station <- station

  values_min <- aggregate( Contribution ~ Date + Type, data = relativeContributions_station, FUN = min )
  values_min$Limit <-"min"
  values_max <- aggregate( Contribution ~ Date + Type, data = relativeContributions_station, FUN = max )
  values_max$Limit <-"max"
  
  contributions_merged <- merge(values_min,values_max, by= c("Date", "Type"))
  
  contributions_merged <- contributions_merged[,c(1,2,3,5)]
  names(contributions_merged)[3] <- "Min"
  names(contributions_merged)[4] <- "Max"
  
  contributions_merged$Type <- factor(contributions_merged$Type, 
                                      levels = c('DIN', 
                                                 "PO4", 
                                                 "SiO4",
                                                 "PAR",
                                                 "Temperature",
                                                 "Zooplankton"))
  contributions_plot$Type <- factor(contributions_plot$Type, 
                               levels = c('DIN', 
                                          "PO4", 
                                          "SiO4",
                                          "PAR",
                                          "Temperature",
                                          "Zooplankton"))

  my_labeller <- as_labeller(c(DIN = 'DIN', PO4 = "PO[4]", SiO4 = "SiO[4]", PAR = "PAR", Temperature = "SST", Zooplankton = "Zooplankton~grazing"), default = label_parsed)
  
# Create the graph
comparisonGraph <- ggplot() +
    geom_line(data = contributions_plot, aes(x = as.Date(Date), y = percentage), size = 0.8) + 
    geom_ribbon(data = contributions_merged, aes(x=as.Date(Date), ymin = Min, ymax = Max),  alpha = 0.15) +
    ylim(0,0.5) +
    facet_wrap(vars(Type), scales = "free_y", labeller = labeller(Type =  my_labeller))+
    scale_x_date(date_breaks = "3 months", date_labels =  "%b %Y", expand = c(0, 0)) +
    theme(text = element_text(size=7), axis.text.x=element_text(angle=90, hjust=1)) +
    theme(legend.position = "right",legend.direction = "vertical",legend.background = element_rect(),legend.title = element_text(size=14,face = 'bold'),legend.text = element_text(size=12),legend.text.align = 0,axis.text.x = element_text(colour="black",size=12),
 axis.text.y = element_text(colour="black",size=14), axis.title.x = element_text(colour="black",size=16,face = 'bold'),
 axis.title.y = element_text(colour="black",size=16,face = 'bold'), strip.text = element_text(size = 12))+
    labs(x = "Date", y= "Averaged monthly relative contribution")

comparisonGraph
```
In the next example, the relative contribution are visualised by boxplots.
```{r, echo=FALSE}
####################################
#Summary Relative contributions (boxplots)
comparisonContribution <- ggplot() +
  geom_boxplot(data = contributions_plot, aes(x = as.factor(Type), y = percentage),outlier.shape = NA) + 
  ylim(0,0.45 )+
  theme(text = element_text(size=12), axis.text.x=element_text(angle=45, hjust=1)) +
  theme(legend.position = "none",legend.direction = "vertical",legend.background = element_rect(),legend.title = element_text(size=14,face = 'bold'),
        legend.text = element_text(size=12),legend.text.align = 0,axis.text.x = element_text(colour="black",size=11.5),
        axis.text.y = element_text(colour="black",size=14), axis.title.x = element_text(colour="black",size=16,face = 'bold'),
        axis.title.y = element_text(colour="black",size=16,face = 'bold'), strip.text = element_text(size = 12))+
  labs(x = "Region", y= "Averaged monthly relative Contribution") 

comparisonContribution
```

Let's save our graphs in a subfolder called 'Graphs' in the 'Output' folder.
```{r, echo=FALSE}
dir.create(paste0(wd, "Output/Graphs")) # create an subfolder for your graphs
ggsave("Phyto- and zooplankton dynamics.jpeg", plot = comparisonGraphAvg_station_chla_zoo, path = paste0(wd, "Output/Graphs"),
       width = 28,  height = 14,  units = "cm", dpi = 300)
ggsave("Phytoplankton dynamics.jpeg", plot = comparisonGraphAvg_station_chla, path = paste0(wd, "Output/Graphs"),
       width = 28,  height = 14,  units = "cm", dpi = 300)
ggsave("Zooplankton dynamics.jpeg", plot = comparisonGraphAvg_station_zoo, path = paste0(wd, "Output/Graphs"),
       width = 28,  height = 14,  units = "cm", dpi = 300)
ggsave("Relative contributions.jpeg", plot = comparisonGraph, path = paste0(wd, "Output/Graphs"),
       width = 28,  height = 14,  units = "cm", dpi = 300)
ggsave("Boxplot relative contributions.jpeg", plot = comparisonContribution, path = paste0(wd, "Output/Graphs"),
       width = 20,  height = 14,  units = "cm", dpi = 300)
```


That's all folks!!


# 4. References

Everaert, G., De Laender, F., Goethals, P.L.M, Janssen, C.R (2015). Relative contribution of persistent organic pollutants to marine phytoplankton biomass dynamics in the North Sea and the Kattegat. Chemosphere 134, 76-83. http://dx.doi.org/10.1016/j.chemosphere.2015.03.084

Soetaert, K., Herman, P.M.J. (2009). A practical guide to ecological modelling. Using R as a Simulation Platform. Springer-Verlag, New York, US, p. 54 - 58.

Van Ginderdeuren, K., Van Hoey, G., Vincx, M. and Hostens, K. (2014). The mesozooplankton community of the Belgian shelf (North Sea). Journal of Sea Research 85, 48 - 58. http://dx.doi.org/10.1016/j.seares.2013.10.003

## Data sets
Flanders Marine Institute (VLIZ): Marine Information and Data Acquisition System: Underway & Cruise data, 2017.

Flanders Marine Institute (VLIZ): LifeWatch observatory data: phytoplankton observations by imaging flow cytometry (FlowCam) in the Belgian Part of the North Sea., doi:https://doi.org/10.14284/330, 2019.

Flanders Marine Institute (VLIZ): LifeWatch observatory data: nutrient, pigment, suspended matter and secchi measurements in the Belgian Part of the North Sea., doi:https://doi.org/10.14284/441, 2021.

Flanders Marine Institute (VLIZ): LifeWatch observatory data: zooplankton observations in the Belgian Part of the North Sea., doi:https://doi.org/10.14284/445, 2021.

Mortelmans, J., Deneudt, K., Cattrijsse, A., Beauchard, O., Daveloose, I., Vyverman, W., Vanaverbeke, J., Timmermans, K., Peene, J., Roose, P., Knockaert, M., Chou, L., Sanders, R., Stinchcombe, M., Kimpe, P., Lammens, S., Theetaert, H., Gkritzalis, T., Hernandez, F. and Mees, J.: Nutrient, pigment, suspended matter and turbidity measurements in the Belgian part of the North Sea, Sci. Data, 6(1), 1–8, doi:10.1038/s41597-019-0032-7, 2019.

Mortelmans, J., Aubert, A., Reubens, J., Otero, V., Deneudt, K. and Mees, J.: Copepods (Crustacea: Copepoda) in the Belgian part of the North Sea: Trends, dynamics and anomalies, J. Mar. Syst., 220(April), doi:10.1016/j.jmarsys.2021.103558, 2021.



